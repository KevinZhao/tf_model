{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import TAF\n",
    "import datetime\n",
    "import talib \n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "factors = pd.read_csv('HS300_15m.csv')\n",
    "\n",
    "index = factors['index']\n",
    "High = factors.high.values\n",
    "Low = factors.low.values\n",
    "Close = factors.close.values\n",
    "Open = factors.open.values\n",
    "Volume = factors.volume.values\n",
    "\n",
    "factors = TAF.get_factors(index, Open, Close, High, Low, Volume, drop=True)\n",
    "\n",
    "factors = factors.iloc[-700 * 16 - 11 * 16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始时间 2014-02-25\n",
      "结束时间 2016-12-30\n"
     ]
    }
   ],
   "source": [
    "start_date = factors.index[11*16][:10]\n",
    "end_date = factors.index[-1][:10]\n",
    "\n",
    "print ('开始时间', start_date)\n",
    "print ('结束时间', end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rolling = 88\n",
    "\n",
    "targets = pd.read_csv('HS300_1d.csv')\n",
    "targets.rename(columns={'Unnamed: 0':'tradeDate'}, inplace=True)\n",
    "targets['returns'] = targets.close.shift(-5)/ targets.close - 1.\n",
    "targets['labels'] = 1\n",
    "targets['upper_boundary']= targets.returns.rolling(rolling).mean() + 0.5 * targets.returns.rolling(rolling).std()\n",
    "targets['lower_boundary']= targets.returns.rolling(rolling).mean() - 0.5 * targets.returns.rolling(rolling).std()\n",
    "\n",
    "targets.dropna(inplace=True)\n",
    "targets.loc[targets['returns']>=targets['upper_boundary'], 'labels'] = 2\n",
    "targets.loc[targets['returns']<=targets['lower_boundary'], 'labels'] = 0\n",
    "\n",
    "targets.set_index('tradeDate', inplace=True)\n",
    "targets= targets.loc[start_date:end_date, 'labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  输入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = np.array(factors).reshape(-1, 1, 58)\n",
    "\n",
    "def dense_to_one_hot(labels_dense):\n",
    "    \"\"\"标签 转换one hot 编码\n",
    "    输入labels_dense 必须为非负数\n",
    "    2016-11-21\n",
    "    \"\"\"\n",
    "    num_classes = len(np.unique(labels_dense)) # np.unique 去掉重复函数\n",
    "    raws_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(raws_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((raws_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot  \n",
    "\n",
    "targets = dense_to_one_hot(targets)\n",
    "targets = np.expand_dims(targets, axis=1)\n",
    "\n",
    "train_inputs = inputs[:-100*16]\n",
    "test_inputs = inputs[-100*16 - 11 * 16:]\n",
    "\n",
    "train_targets = targets[:-100]\n",
    "test_targets = targets[-100:]\n",
    "\n",
    "train_gather_list = np.arange(train_inputs.shape[0])\n",
    "train_gather_list = train_gather_list.reshape([-1,16])[11:]\n",
    "train_gather_list = train_gather_list[:,-1]\n",
    "\n",
    "test_gather_list = np.arange(test_inputs.shape[0])\n",
    "test_gather_list = test_gather_list.reshape([-1,16])[11:]\n",
    "test_gather_list = test_gather_list[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1层LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from FixDNCore import DNCore_L1\n",
    "from FixACT import ACTCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier_DNC_BasicLSTM_L1(object):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 inputs, \n",
    "                 targets,\n",
    "                 gather_list=None,\n",
    "                 batch_size=1, \n",
    "                 hidden_size=10, \n",
    "                 memory_size=10, \n",
    "                 threshold=0.99,\n",
    "                 pondering_coefficient = 1e-2,\n",
    "                 num_reads=3,\n",
    "                 num_writes=1):\n",
    "        \n",
    "        self._tmp_inputs = inputs\n",
    "        self._tmp_targets = targets\n",
    "        self._in_length = inputs.shape[0]\n",
    "        self._in_width = inputs.shape[2]\n",
    "        self._out_length = targets.shape[0]\n",
    "        self._out_width = targets.shape[2]\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "        # \n",
    "        self._sess = tf.InteractiveSession()\n",
    "        \n",
    "        self._inputs = tf.placeholder(dtype=tf.float32, \n",
    "                                      shape=[self._in_length, self._batch_size, self._in_width], \n",
    "                                      name='inputs')\n",
    "        self._targets = tf.placeholder(dtype=tf.float32, \n",
    "                                       shape=[self._out_length, self._batch_size, self._out_width],\n",
    "                                       name='targets')\n",
    "        \n",
    "        act_core = DNCore_L1( hidden_size=hidden_size, \n",
    "                              memory_size=memory_size, \n",
    "                              word_size=self._in_width, \n",
    "                              num_read_heads=num_reads, \n",
    "                              num_write_heads=num_writes)        \n",
    "        self._InferenceCell = ACTCore(core=act_core, \n",
    "                                      output_size=self._out_width, \n",
    "                                      threshold=threshold, \n",
    "                                      get_state_for_halting=self._get_hidden_state)\n",
    "        \n",
    "        self._initial_state = self._InferenceCell.initial_state(self._batch_size)\n",
    "        \n",
    "        tmp, act_final_cumul_state = \\\n",
    "        tf.nn.dynamic_rnn(cell=self._InferenceCell, \n",
    "                          inputs=self._inputs, \n",
    "                          initial_state=self._initial_state, \n",
    "                          time_major=True)\n",
    "        act_output, (act_final_iteration, act_final_remainder) = tmp\n",
    "        \n",
    "        self._pred_outputs = act_output\n",
    "        if gather_list is not None:\n",
    "            out_sequences = tf.gather(act_output, gather_list)\n",
    "        else:\n",
    "            out_sequences = act_core\n",
    "        \n",
    "        pondering_cost = (act_final_iteration + act_final_remainder) * pondering_coefficient\n",
    "        rnn_cost = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=self._targets, logits=out_sequences)\n",
    "        self._cost = tf.reduce_mean(rnn_cost) + tf.reduce_mean(pondering_cost)\n",
    "        \n",
    "        self._pred = tf.nn.softmax(out_sequences, dim=2)\n",
    "        correct_pred = tf.equal(tf.argmax(self._pred,2), tf.argmax(self._targets,2))\n",
    "        self._accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "    def _get_hidden_state(self, state):\n",
    "        controller_state = state[0]\n",
    "        next_state, next_cell = controller_state\n",
    "        return next_state\n",
    "        \n",
    "    def fit(self, \n",
    "            training_iters =1e2,             \n",
    "            learning_rate = 1e-4,\n",
    "            optimizer_epsilon = 1e-10,\n",
    "            max_gard_norm = 50):\n",
    "\n",
    "        # Set up optimizer with global norm clipping.\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(self._cost, trainable_variables), max_gard_norm)\n",
    "        global_step = tf.get_variable(\n",
    "            name=\"global_step\",\n",
    "            shape=[],\n",
    "            dtype=tf.int64,\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            trainable=False,\n",
    "            collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n",
    "        \n",
    "        optimizer = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=learning_rate, epsilon=optimizer_epsilon)\n",
    "        self._train_step = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables), global_step=global_step)  \n",
    "        \n",
    "        self._sess.run(tf.global_variables_initializer())\n",
    "        for scope in range(np.int(training_iters)):\n",
    "            _, loss, acc = self._sess.run([self._train_step, self._cost, self._accuracy], \n",
    "                                     feed_dict = {self._inputs:self._tmp_inputs, \n",
    "                                                  self._targets:self._tmp_targets})\n",
    "            print (scope, '  loss--', loss, '  acc--', acc)\n",
    "        print (\"Optimization Finished!\") \n",
    "            \n",
    "            \n",
    "    def close(self):\n",
    "        self._sess.close()\n",
    "        print ('结束进程，清理tensorflow内存/显存占用')\n",
    "        \n",
    "        \n",
    "    def pred(self, inputs, gather_list=None):\n",
    "        \n",
    "        output_pred = self._pred_outputs\n",
    "        if gather_list is not None:\n",
    "            output_pred = tf.gather(output_pred, gather_list)\n",
    "        probability = tf.nn.softmax(output_pred)\n",
    "        classification = tf.argmax(probability, axis=-1)\n",
    "        \n",
    "        return self._sess.run([probability, classification],feed_dict = {self._inputs:inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   loss-- 1.12566   acc-- 0.338333\n",
      "1   loss-- 1.12085   acc-- 0.353333\n",
      "2   loss-- 1.11663   acc-- 0.378333\n",
      "3   loss-- 1.11299   acc-- 0.391667\n",
      "4   loss-- 1.1098   acc-- 0.395\n"
     ]
    }
   ],
   "source": [
    "a = Classifier_DNC_BasicLSTM_L1(train_inputs, train_targets, train_gather_list)\n",
    "a.fit(5,learning_rate = 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b,c = a.pred(train_inputs, train_gather_list)\n",
    "\n",
    "b = np.squeeze(b)\n",
    "\n",
    "b = pd.DataFrame(b)\n",
    "\n",
    "t = np.argmax(train_targets, axis=-1)\n",
    "\n",
    "tmp = pd.DataFrame([t.flatten(),c.flatten()]).T\n",
    "\n",
    "tmp.columns = ['targets','pred']\n",
    "\n",
    "tmp = pd.concat([tmp, b], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>targets</th>\n",
       "      <th>pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.257946</td>\n",
       "      <td>0.373783</td>\n",
       "      <td>0.368270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.252376</td>\n",
       "      <td>0.336031</td>\n",
       "      <td>0.411593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.281975</td>\n",
       "      <td>0.368138</td>\n",
       "      <td>0.349887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.321836</td>\n",
       "      <td>0.351986</td>\n",
       "      <td>0.326178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.368033</td>\n",
       "      <td>0.334293</td>\n",
       "      <td>0.297674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   targets  pred         0         1         2\n",
       "0        2     1  0.257946  0.373783  0.368270\n",
       "1        1     2  0.252376  0.336031  0.411593\n",
       "2        2     1  0.281975  0.368138  0.349887\n",
       "3        1     1  0.321836  0.351986  0.326178\n",
       "4        0     0  0.368033  0.334293  0.297674"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3层LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "from FixDNCore import DNCore_L3\n",
    "from FixACT import ACTCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier_DNC_BasicLSTM_L3(object):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 inputs, \n",
    "                 targets,\n",
    "                 gather_list=None,\n",
    "                 batch_size=1, \n",
    "                 hidden_size=10, \n",
    "                 memory_size=10, \n",
    "                 threshold=0.99,\n",
    "                 pondering_coefficient = 1e-2,\n",
    "                 num_reads=3,\n",
    "                 num_writes=1):\n",
    "        \n",
    "        self._tmp_inputs = inputs\n",
    "        self._tmp_targets = targets\n",
    "        self._in_length = None\n",
    "        self._in_width = inputs.shape[2]\n",
    "        self._out_length = None\n",
    "        self._out_width = targets.shape[2]\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "        # \n",
    "        self._sess = tf.InteractiveSession()\n",
    "        \n",
    "        self._inputs = tf.placeholder(dtype=tf.float32, \n",
    "                                      shape=[self._in_length, self._batch_size, self._in_width], \n",
    "                                      name='inputs')\n",
    "        self._targets = tf.placeholder(dtype=tf.float32, \n",
    "                                       shape=[self._out_length, self._batch_size, self._out_width],\n",
    "                                       name='targets')\n",
    "        \n",
    "        act_core = DNCore_L3( hidden_size=hidden_size, \n",
    "                              memory_size=memory_size, \n",
    "                              word_size=self._in_width, \n",
    "                              num_read_heads=num_reads, \n",
    "                              num_write_heads=num_writes)        \n",
    "        self._InferenceCell = ACTCore(core=act_core, \n",
    "                                      output_size=self._out_width, \n",
    "                                      threshold=threshold, \n",
    "                                      get_state_for_halting=self._get_hidden_state)\n",
    "        \n",
    "        self._initial_state = self._InferenceCell.initial_state(self._batch_size)\n",
    "        \n",
    "        tmp, act_final_cumul_state = \\\n",
    "        tf.nn.dynamic_rnn(cell=self._InferenceCell, \n",
    "                          inputs=self._inputs, \n",
    "                          initial_state=self._initial_state, \n",
    "                          time_major=True)\n",
    "        act_output, (act_final_iteration, act_final_remainder) = tmp\n",
    "        \n",
    "        self._pred_outputs = act_output\n",
    "        if gather_list is not None:\n",
    "            out_sequences = tf.gather(act_output, gather_list)\n",
    "        else:\n",
    "            out_sequences = act_core\n",
    "        \n",
    "        pondering_cost = (act_final_iteration + act_final_remainder) * pondering_coefficient\n",
    "        rnn_cost = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=self._targets, logits=out_sequences)\n",
    "        self._cost = tf.reduce_mean(rnn_cost) + tf.reduce_mean(pondering_cost)\n",
    "        \n",
    "        self._pred = tf.nn.softmax(out_sequences, dim=2)\n",
    "        correct_pred = tf.equal(tf.argmax(self._pred,2), tf.argmax(self._targets,2))\n",
    "        self._accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "    # 待处理函数\n",
    "    def _get_hidden_state(self, state):\n",
    "        controller_state, access_state, read_vectors = state\n",
    "        layer_1, layer_2, layer_3 = controller_state\n",
    "        L1_next_state, L1_next_cell = layer_1\n",
    "        L2_next_state, L2_next_cell = layer_2\n",
    "        L3_next_state, L3_next_cell = layer_3\n",
    "        return tf.concat([L1_next_state, L2_next_state, L3_next_state], axis=-1)\n",
    "        \n",
    "    def fit(self, \n",
    "            training_iters =1e2,             \n",
    "            learning_rate = 1e-4,\n",
    "            optimizer_epsilon = 1e-10,\n",
    "            max_gard_norm = 50):\n",
    "\n",
    "        # Set up optimizer with global norm clipping.\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(self._cost, trainable_variables), max_gard_norm)\n",
    "        global_step = tf.get_variable(\n",
    "            name=\"global_step\",\n",
    "            shape=[],\n",
    "            dtype=tf.int64,\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            trainable=False,\n",
    "            collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n",
    "        \n",
    "        optimizer = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=learning_rate, epsilon=optimizer_epsilon)\n",
    "        self._train_step = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables), global_step=global_step)  \n",
    "        \n",
    "        self._sess.run(tf.global_variables_initializer())\n",
    "        for scope in range(np.int(training_iters)):\n",
    "            _, loss, acc = self._sess.run([self._train_step, self._cost, self._accuracy], \n",
    "                                     feed_dict = {self._inputs:self._tmp_inputs, \n",
    "                                                  self._targets:self._tmp_targets})\n",
    "            print (scope, '  loss--', loss, '  acc--', acc)\n",
    "        print (\"Optimization Finished!\") \n",
    "            \n",
    "            \n",
    "    def close(self):\n",
    "        self._sess.close()\n",
    "        print ('结束进程，清理tensorflow内存/显存占用')\n",
    "        \n",
    "        \n",
    "    def pred(self, inputs, gather_list=None):\n",
    "        \n",
    "        output_pred = self._pred_outputs\n",
    "        if gather_list is not None:\n",
    "            output_pred = tf.gather(output_pred, gather_list)\n",
    "        probability = tf.nn.softmax(output_pred)\n",
    "        classification = tf.argmax(probability, axis=-1)\n",
    "        \n",
    "        return self._sess.run([probability, classification],feed_dict = {self._inputs:inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `skip_connections` argument will be deprecated. Please use snt.SkipConnectionCore instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   loss-- 1.14674   acc-- 0.283333\n",
      "1   loss-- 1.13386   acc-- 0.331667\n",
      "2   loss-- 1.12391   acc-- 0.358333\n",
      "3   loss-- 1.11623   acc-- 0.388333\n",
      "4   loss-- 1.11026   acc-- 0.4\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "la = Classifier_DNC_BasicLSTM_L3(train_inputs, train_targets, train_gather_list)\n",
    "la.fit(5,learning_rate = 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>targets</th>\n",
       "      <th>pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.255015</td>\n",
       "      <td>0.369511</td>\n",
       "      <td>0.375474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.276327</td>\n",
       "      <td>0.344161</td>\n",
       "      <td>0.379513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.336804</td>\n",
       "      <td>0.334562</td>\n",
       "      <td>0.328634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.276730</td>\n",
       "      <td>0.370372</td>\n",
       "      <td>0.352898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333317</td>\n",
       "      <td>0.377212</td>\n",
       "      <td>0.289471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   targets  pred         0         1         2\n",
       "0        2     2  0.255015  0.369511  0.375474\n",
       "1        1     2  0.276327  0.344161  0.379513\n",
       "2        2     0  0.336804  0.334562  0.328634\n",
       "3        1     1  0.276730  0.370372  0.352898\n",
       "4        0     1  0.333317  0.377212  0.289471"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b,c = la.pred(train_inputs, train_gather_list)\n",
    "\n",
    "b = np.squeeze(b)\n",
    "\n",
    "b = pd.DataFrame(b)\n",
    "\n",
    "t = np.argmax(train_targets, axis=-1)\n",
    "\n",
    "tmp = pd.DataFrame([t.flatten(),c.flatten()]).T\n",
    "\n",
    "tmp.columns = ['targets','pred']\n",
    "\n",
    "tmp = pd.concat([tmp, b], axis=1)\n",
    "\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
